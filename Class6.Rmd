---
title: "Class6"
author: "Kyle Mahowald"
date: "2024-10-06"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
d = read_csv("english.csv")
d$RTms = exp(d$RTlexdec)
d$WrittenFrequencyOriginal = exp(d$WrittenFrequency)

hist(d$WrittenFrequencyOriginal, breaks=1000)

ggplot(d, aes(x=WrittenFrequencyOriginal, y=RTms)) + 
  geom_point() +
  geom_smooth(method=lm)
```
# Logs
```{r}
log2(8)
# 2 * 2 = 4
# 2 * 2 * 2 = 8
x = seq(1, 1000)
y = log(x)
plot(x, y)

ggplot(d, aes(x=(WrittenFrequencyOriginal), y=RTms)) + 
  geom_point() +
  geom_smooth(method=lm)


l.0 = lm(RTms ~ WrittenFrequencyOriginal, data=d)
l.1 = lm(RTms ~ WrittenFrequency, data=d)

summary(l.0)
summary(l.1)
# log10
# log2
# log e

d$DistFromChance = abs(d$CorrectLexdec - 15)
ggplot(d, aes(x=CorrectLexdec, y=RTms, label=Word)) + 
  geom_text() + geom_smooth(method=lm)

ggplot(d, aes(x=DistFromChance, y=RTms, label=Word, colour=AgeSubject)) + 
  geom_text() + geom_smooth(method=lm)
```

# Multiple Regression, with likelihood ratio test (chi-squared test)

```{r}
l1 = lm(data=d, RTms ~ Familiarity)
l2 = lm(data=d, RTms ~ Familiarity + LengthInLetters) 
summary(l2)
# familiarity = 4
# 3 letter word
870.286 + (4 * -44.208) + (3 * 1.298) 

l3 = lm(data=d, RTms ~ Familiarity + AgeSubject)
# AgeSubject: old/young
# prediction for Familiarity of 4 and AgeSubject == young
# young == 1, old == 0
954.7980 +  (4 * -44.286) + (1 * -157.1567)
954.7980 +  (4 * -44.286) 


l4 = lm(data=d, RTms ~ Familiarity + AgeSubject + LengthInLetters + DistFromChance) 

anova(l1, l2)
anova(l1, l3)

d$L4Prediction = predict(l4)
ggplot(d, aes(x=L4Prediction, y=RTms, label=Word)) + geom_text()

# make a prediction using l3
```

# Logistic Regression

Predict if a row is from a young or old subject

```{r}
d$AgeBinary = (d$AgeSubject == "young")
summary(glm(data=d,
           family="binomial",
           AgeBinary ~ WrittenFrequency))

summary(glm(data=d,
           family="binomial",
           AgeBinary ~ RTms))

# model prediction for when RTms is 600
19.366 - .0277 * 600

logistic = function(x) {1 / (1 + exp(-x))}
logit = function(p) {log(p/(1-p))}

logistic(19.366 - .0277 * 600) # probability of getting a 1

logistic(19.366 - .0277 * 0) # probability of getting a 1


logit(.9)

x = seq(0, 1, .001)
y = logit(x)

plot(x, y)
plot(y, x)
```

# Zscores and Correlation
https://istics.net/Correlations/

A correlation.... a regression with standardized variables!

```{r}
d$Familiarity.Z = (d$Familiarity - mean(d$Familiarity))/sd(d$Familiarity)
d$RTms.Z = (d$RTms - mean(d$RTms))/sd(d$RTms)

hist(d$RTms.Z)
summary(lm(data=d, RTms.Z ~ Familiarity.Z))
cor(d$Familiarity.Z, d$RTms.Z)


```