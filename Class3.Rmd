---
title: "Class3"
author: "Kyle Mahowald"
date: "2024-09-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ggplot Review

Read in our english.csv data. Use summarize to create a data frame in which you find the average RTms for each Word (averaging over young and old subjects).

Now plot the relationship between LengthInLetters and the mean RTms. 

```{r}
library(tidyverse)
d = read_csv("english.csv")
d$RTms = exp(d$RTlexdec)

mean(d$RTms)

# how many RTms per each word
table(d$Word)

d.grouped = group_by(d, Word, LengthInLetters, WordCategory) %>%
  summarise(meanRT = mean(RTms))

table(d.grouped$Word)

ggplot(d.grouped, aes(x=LengthInLetters, y=meanRT)) +
  geom_point()

ggplot(d.grouped, aes(x=LengthInLetters, y=meanRT)) +
  geom_jitter()

ggplot(d.grouped, aes(x=LengthInLetters, y=meanRT)) +
  geom_boxplot()

ggplot(d.grouped, aes(x=as.factor(LengthInLetters), y=meanRT, colour=as.factor(LengthInLetters))) +
  geom_boxplot() +
  geom_jitter(alpha=.1) +
  theme_classic(12) +
  xlab("How many letters is the word?") +
  ylab("mean reaction time in milliseconds") +
  facet_wrap(~WordCategory)


```


# Rearranging DataFrames

Let's look at words that are especially different between young and old subjects Let's do it by creating a data frame with these columns: Word,Young,Old where the values are RTms.

Do it for just nouns!

pivot_wider()

pivot_longer()

```{r}

filter(d, Word == "deer")



select(d, Word, AgeSubject, RTms, WordCategory, CorrectLexdec) %>%
  filter(WordCategory == "N") %>%
  pivot_wider(names_from = c(AgeSubject),
              values_from = c(RTms, CorrectLexdec)) %>%
  mutate(AgeDiff = RTms_old - RTms_young) %>%
  arrange(-AgeDiff) %>%
  ggplot(aes(x=CorrectLexdec_young, y=RTms_young)) + geom_jitter() +
  geom_smooth(method=lm, colour="red") + 
  geom_smooth() +
  xlab("Number correct out of 30") + 
  ylab("mean reaction time in ms for young subjects") +
  theme_classic(18)
  
```

# How different is our data from random?

```{r}
ggplot(d, aes(x=WrittenFrequency, y=RTms, colour=AgeSubject)) +
  geom_point()

x = runif(1000)
y = runif(1000)
plot(x, y)

group_by(d, AgeSubject) %>%
  summarise(m=mean(RTms))
```

# The mean as a model

```{r}
d.1 = d %>%
  group_by(AgeSubject) %>%
  mutate(Prediction1 = mean(RTms))

d.1 = d.1 %>%
  ungroup() %>%
  mutate(Prediction0 = mean(RTms))

d.1 = d.1 %>%
  mutate(Error0 = RTms - Prediction0,
         Error1 = RTms - Prediction1)

# Mean Absolute Error
mean(abs(d.1$Error0))
mean(abs(d.1$Error1))

# Sum Squared Error
sum((d.1$Error0)^2) > sum((d.1$Error1)^2)


a = NULL

for (i in 1:1000) {
  d.1$RandomAge = sample(d.1$AgeSubject)
d.1 = d.1 %>%
  group_by(RandomAge) %>%
  mutate(PredictionRandom = mean(RTms),
         ErrorRandom = RTms - PredictionRandom)

a = append(a, sum((d.1$ErrorRandom)^2) > sum((d.1$Error1)^2))

}
mean(a)

# permutation test


# sum((d.1$ErrorRandom)^2) > sum((d.1$Error0)^2)


```


# Starting some summary stats: Understanding mean, median, and standard deviation (sd)

```{r}
x = c(1, 2, 3, 4, 5)
y = c(1, 2, 3, 4, 100000000)
```

# Standard deviation

On average, how far away is a value from the mean?

```{r}
```


# Computing the error of our estimate


```{r}
```

# Uniform and normal

```{r}
u = runif(10000, 0, 1)
n = rnorm(10000, 0, 1)

d = tibble(unif=u, norm=n)
```

# Make some different histograms by taking the mean of the uniform distribution. 

```{r}
```

# Exercise

Take 10000 draws from a uniform distribution [0, 1]. Find the 2.5 percentile, 50th percentile, 97.5th percentile empirically (by computing it). That is: find the value for which 2.5% of the data is less than that value. Then for 50% (this is the median!), etc. 

Now do the same for a normal distribution. Keep the mean 0, but play around with the standard deviation (try 1, 2, 3). How does this affect things?

```{r}
```


# Exercise 2


```{r}
d = read_csv("../textbook/data/perry_winter_2017_iconicity.csv")
arrange(d, -Iconicity) %>%
  select(Word, POS, Iconicity)

arrange(d, Iconicity) %>%
  select(Word, POS, Iconicity)
```


Get the iconicity scores from d above. Rank them, look at them.

Now make a histogram. Find the mean and standard deviation.

```{r, width=3, height=3}
hist(d$Freq, breaks=50)
hist(d$Iconicity)

icon.mean = mean(d$Iconicity)
icon.sd = sd(d$Iconicity)
icon.nrow = nrow(d)
```

Now sample a normal distribution with that mean and standard deviation. 
How similar are they?

```{r}
print(hist(rnorm(icon.nrow, icon.mean, icon.sd), breaks=50))
print(hist(d$Iconicity, breaks=50))

icon = tibble(icon=d$Iconicity, type="real")
sampled = tibble(icon=rnorm(icon.nrow, icon.mean, icon.sd), type="sim")
icon.sampled = bind_rows(icon, sampled)
ggplot(icon.sampled, aes(x=icon, fill=type)) +
  geom_histogram(alpha=.4)  +
  facet_grid(type ~ .)

nrow(filter(icon.sampled, type == "real"))
nrow(filter(icon.sampled, type == "sim"))

```

# Mean as a model

Task: for a random word, predict the iconicity.

Idea: Use the mean.

```{r}
d$Prediction = mean(d$Iconicity)
```

How wrong will we be!? (I.e.: what is the error?)

```{r}
d$Residual = d$Iconicity - d$Prediction
sum(d$Residual) # this is close to 0 because we have a mixture of negatives and positives
sum(abs(d$Residual)) # absolute total error
sum(abs(d$Residual))/nrow(d)
```

How wrong will we be (squared)?

Instead of absolute difference, look at squared difference.

Mean Squared Error is a crucial quantity for linear regression and statistics.

```{r}
d$SquareDiff = d$Residual * d$Residual
hist(d$SquareDiff)
sum(d$SquareDiff)
sum(d$SquareDiff)/nrow(d)
```

Now do it for POS I.e., compare POS == "Interjection" vs. POS == "Name"

Use Mean Squared Error to assess how wrong we are.

Using group means minimizes least square error MORE than the overall mean.

```{r}
group_by(d, POS) %>%
  summarise(icon.mean=mean(Iconicity))

d = group_by(d,POS) %>%
  mutate(pos.mean = mean(Iconicity))

d$Residual.Pos = d$pos.mean - d$Iconicity
d$SquareDiff.Pos = d$Residual.Pos * d$Residual.Pos

# using overall mean
sum(d$SquareDiff)/nrow(d)

# using group means
sum(d$SquareDiff.Pos)/nrow(d)
```

# Overfitting with coins

```{r}
rbinom(3, 1, .5)
```
